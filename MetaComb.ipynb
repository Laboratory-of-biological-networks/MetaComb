{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb086e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_auc_score, roc_curve,precision_recall_curve, auc\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf75d3",
   "metadata": {},
   "source": [
    "## Loading processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4475bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell line level data\n",
    "df =  pd.read_pickle(\"E:/data/cl_label_data.pickle\") \n",
    "drug_features =  pd.read_pickle('E:/data/drug_feature.pickle')\n",
    "cell_features =  pd.read_pickle('E:/data/cellline_feature.pickle')\n",
    "cuda=True\n",
    "device = torch.device('cuda'if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#patient level data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e652a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6042b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two layers of fully connected layers\n",
    "class FC2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(FC2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, int(in_features/2))\n",
    "        self.fc2 = nn.Linear(int(in_features/2),out_features)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#Classification predictor\n",
    "class COMBFC2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(COMBFC2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, int(in_features/2))\n",
    "        self.fc2 = nn.Linear(int(in_features/2), int(in_features/2))\n",
    "        self.fc3= nn.Linear(int(in_features/2),out_features)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        self.sigmoid= nn.Sigmoid()\n",
    "                \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#DrugEncoder\n",
    "class DrugEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                drug_sm_conv = 96,\n",
    "                 num_comp_char=48,\n",
    "                 out_size=64,\n",
    "                 dropout=0.3):\n",
    "        super(DrugEncoder, self).__init__()\n",
    "        \n",
    "        self.dropout= dropout\n",
    "        \n",
    "        #smiles embedding \n",
    "        self.embed_comp = nn.Embedding(num_comp_char,num_comp_char, padding_idx=0)#padding's idx=0,\n",
    "        #smiles 卷积\n",
    "        self.conv1 = nn.Conv1d(in_channels=48,out_channels=32,kernel_size=4) ##stride=1\n",
    "        self.conv2 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=6)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64,out_channels=96,kernel_size=8)\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1) \n",
    " \n",
    "     \n",
    "        self.FC2 = FC2(96, out_size, dropout)\n",
    "        \n",
    "    def forward(self, d_list):\n",
    "        \"\"\"\n",
    "            id: bsz*1\n",
    "            fp: bsz*num_drug_fp\n",
    "            sm: bsz*drug_sm_len\n",
    "        \"\"\"\n",
    "        id, sm = d_list\n",
    "        \n",
    "        sm = self.embed_comp(sm) #becomes 2-dimensional after embedding\n",
    "#         sm = sm.reshape(bsz,drug_sm_len,num_comp_char) \n",
    "        sm = sm.permute(0,2,1) #Dimension conversion (batch size,in_channels: dimension of word vector, data length: sentence length)\n",
    "        sm = F.relu(self.conv1(sm))\n",
    "        sm = F.relu(self.conv2(sm))\n",
    "        sm = F.relu(self.conv3(sm))\n",
    "        sm = self.maxpool(sm)\n",
    "        sm = sm.squeeze(-1) \n",
    "        \n",
    "        \n",
    "        x = self.FC2(sm)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "#CellEncoder      \n",
    "class CellEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "               out_size=64,\n",
    "               dropout=0.3):\n",
    "        super(CellEncoder,self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(14753,2950)\n",
    "        self.fc2 = nn.Linear(2950,590)\n",
    "        self.fc3 = nn.Linear(590,64)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,c_list):\n",
    "        id,ge  = c_list\n",
    "        \n",
    "        ge = self.dropout(ge)\n",
    "        ge = self.dropout(F.relu(self.fc1(ge)))\n",
    "        ge = self.dropout(F.relu(self.fc2(ge)))\n",
    "        x = self.fc3(ge)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Comb(nn.Module):\n",
    "    def __init__(self,\n",
    "              out_size = 64,\n",
    "              dropout = 0.3):\n",
    "        super(Comb, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        #drug \n",
    "        self.DrugEncoder = DrugEncoder()\n",
    "        #cell\n",
    "        self.CellEncoder = CellEncoder()\n",
    "        #fc\n",
    "        self.fc_response = COMBFC2(out_size*3, 1, dropout) \n",
    "        \n",
    "    def forward(self,d1_list,d2_list,c_list):\n",
    "        d1 = self.DrugEncoder(d1_list)\n",
    "        d2 = self.DrugEncoder(d2_list)\n",
    "        c = self.CellEncoder(c_list)\n",
    "        alll = torch.cat((d1, d2, c),1)\n",
    "        y = self.fc_response(alll)\n",
    "        \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b58d5b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd5ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##取样本及其对应特征\n",
    "class DrugCombDataset(Dataset):\n",
    "    def __init__(self, df, drug_features, cell_features):\n",
    "        self.df = df\n",
    "        self.drug_features = drug_features\n",
    "        self.cell_features = cell_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        d1 = self.df.iloc[idx, 0]\n",
    "        d2 = self.df.iloc[idx, 1]\n",
    "        cell = self.df.iloc[idx,2]\n",
    "        label = self.df.iloc[idx,3]\n",
    "       \n",
    "        \n",
    "        #external feature\n",
    "        d1_sm = np.array(self.drug_features.loc[d1,'smiles'])\n",
    "\n",
    "        \n",
    "        d2_sm = np.array(self.drug_features.loc[d2,'smiles'])\n",
    "\n",
    "        \n",
    "        c_ge= np.array(self.cell_features.iloc[cell][:])  ##表达谱信息放置\n",
    "        \n",
    "        sample = {\n",
    "            'd1': d1,\n",
    "            'd1_sm': d1_sm,\n",
    "\n",
    "            \n",
    "            'd2': d2,\n",
    "            'd2_sm': d2_sm,\n",
    "\n",
    "            \n",
    "            'cell': cell,\n",
    "            'c_ge': c_ge,\n",
    "            \n",
    "#             'css': css,\n",
    "            'label':label\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "singledrug = df['cell_line_name'].value_counts().reset_index()\n",
    "singledrug.columns = ['cell_line_name','frequency']\n",
    "singledrug_s = singledrug[singledrug['frequency'] > 50]\n",
    "singledrug_t = singledrug[(singledrug['frequency'] < 50)&(singledrug['frequency'] > 10) ]\n",
    "source_data = df[df['cell_line_name'].isin(singledrug_s['cell_line_name'])]\n",
    "target_data = df[df['cell_line_name'].isin(singledrug_t['cell_line_name'])]\n",
    "train_tasks = []\n",
    "cell_lines = source_data['cell_line_name'].unique()\n",
    "for cell_line in cell_lines:\n",
    "    task = source_data[source_data['cell_line_name'] == cell_line]\n",
    "    train_tasks.append(task)\n",
    "test_tasks = []\n",
    "cell_lines = target_data['cell_line_name'].unique()\n",
    "for cell_line in cell_lines:\n",
    "    task = target_data[target_data['cell_line_name'] == cell_line]\n",
    "    test_tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e43371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sampling(df,k):\n",
    "    df_positive = df[df['label'] == 1]\n",
    "    df_negative = df[df['label'] == 0]\n",
    "\n",
    "    num_positive = df_positive.shape[0]\n",
    "    num_negative = df_negative.shape[0]\n",
    "\n",
    "    num_positive_sample = min(num_positive, k)\n",
    "    num_negative_sample = min(num_negative, k)\n",
    "\n",
    "    if num_positive_sample < k:\n",
    "        num_negative_sample = min(num_negative, 2*k - num_positive_sample)\n",
    "    elif num_negative_sample < k:\n",
    "        num_positive_sample = min(num_positive, 2*k - num_negative_sample)\n",
    "\n",
    "    sample_positive = df_positive.sample(n=num_positive_sample)\n",
    "    sample_negative = df_negative.sample(n=num_negative_sample)\n",
    "\n",
    "    sample_df = pd.concat([sample_positive, sample_negative])\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbf1b2",
   "metadata": {},
   "source": [
    "## Meta-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a341173",
   "metadata": {},
   "source": [
    "#### 源域的MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "279980dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_meta = Comb().to(device)\n",
    "meta_lr = 1e-3\n",
    "fast_lr = 0.1\n",
    "criterion = nn.BCELoss()\n",
    "maml = l2l.algorithms.MAML(net_meta, lr=fast_lr,allow_unused=True) \n",
    "meta_optimizer = optim.Adam(maml.parameters(), lr=meta_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bcaea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch: 1, Meta Train Loss: 0.787327766418457\n",
      "Patch: 2, Meta Train Loss: 0.76658695936203\n",
      "Patch: 3, Meta Train Loss: 0.7010809779167175\n",
      "Patch: 4, Meta Train Loss: 0.6850055456161499\n",
      "Patch: 5, Meta Train Loss: 0.6884840726852417\n",
      "Patch: 6, Meta Train Loss: 0.6691299676895142\n",
      "Patch: 7, Meta Train Loss: 0.7063966393470764\n",
      "Patch: 8, Meta Train Loss: 0.7058179378509521\n",
      "Patch: 9, Meta Train Loss: 0.7011981010437012\n",
      "Patch: 10, Meta Train Loss: 0.6909726858139038\n",
      "Patch: 11, Meta Train Loss: 0.6830936670303345\n",
      "Patch: 12, Meta Train Loss: 0.6997944116592407\n",
      "Patch: 13, Meta Train Loss: 0.702325165271759\n",
      "Patch: 14, Meta Train Loss: 0.7120388746261597\n",
      "Patch: 15, Meta Train Loss: 0.6953700184822083\n",
      "Patch: 16, Meta Train Loss: 0.6870925426483154\n",
      "Patch: 17, Meta Train Loss: 0.6963696479797363\n",
      "Patch: 18, Meta Train Loss: 0.7059096097946167\n",
      "Patch: 19, Meta Train Loss: 0.6915417313575745\n",
      "Patch: 20, Meta Train Loss: 0.6957017183303833\n",
      "Patch: 21, Meta Train Loss: 0.6971877813339233\n",
      "Patch: 22, Meta Train Loss: 0.6916344165802002\n",
      "Patch: 23, Meta Train Loss: 0.6951352953910828\n",
      "Patch: 24, Meta Train Loss: 0.6967267990112305\n",
      "Patch: 25, Meta Train Loss: 0.6911646723747253\n",
      "Patch: 26, Meta Train Loss: 0.6827206015586853\n",
      "Patch: 27, Meta Train Loss: 0.6907137632369995\n",
      "Patch: 28, Meta Train Loss: 0.6943990588188171\n",
      "Patch: 29, Meta Train Loss: 0.6872966885566711\n",
      "Patch: 30, Meta Train Loss: 0.6872200965881348\n",
      "Patch: 31, Meta Train Loss: 0.6935590505599976\n",
      "Patch: 32, Meta Train Loss: 0.6857336759567261\n",
      "Patch: 33, Meta Train Loss: 0.6950352191925049\n",
      "Patch: 34, Meta Train Loss: 0.6927067637443542\n",
      "Patch: 35, Meta Train Loss: 0.6899897456169128\n",
      "Patch: 36, Meta Train Loss: 0.6954883933067322\n",
      "Patch: 37, Meta Train Loss: 0.6873006820678711\n",
      "Patch: 38, Meta Train Loss: 0.6964329481124878\n",
      "Patch: 39, Meta Train Loss: 0.6862097978591919\n",
      "Patch: 40, Meta Train Loss: 0.6960862874984741\n",
      "Patch: 41, Meta Train Loss: 0.6884113550186157\n",
      "Patch: 42, Meta Train Loss: 0.6859469413757324\n",
      "Patch: 43, Meta Train Loss: 0.6913365125656128\n",
      "Patch: 44, Meta Train Loss: 0.6975650787353516\n",
      "Patch: 45, Meta Train Loss: 0.6926748156547546\n",
      "Patch: 46, Meta Train Loss: 0.6983429193496704\n",
      "Patch: 47, Meta Train Loss: 0.7012671232223511\n",
      "Patch: 48, Meta Train Loss: 0.6900725960731506\n",
      "Patch: 49, Meta Train Loss: 0.6925523281097412\n",
      "Patch: 50, Meta Train Loss: 0.685340404510498\n",
      "Patch: 51, Meta Train Loss: 0.7005247473716736\n",
      "Patch: 52, Meta Train Loss: 0.6803442239761353\n",
      "Patch: 53, Meta Train Loss: 0.6908195614814758\n",
      "Patch: 54, Meta Train Loss: 0.652033805847168\n",
      "Patch: 55, Meta Train Loss: 0.7204083204269409\n",
      "Patch: 56, Meta Train Loss: 0.6921625137329102\n",
      "Patch: 57, Meta Train Loss: 0.691326379776001\n",
      "Patch: 58, Meta Train Loss: 0.6755810379981995\n",
      "Patch: 59, Meta Train Loss: 0.6537565588951111\n",
      "Patch: 60, Meta Train Loss: 0.6615576148033142\n",
      "Patch: 61, Meta Train Loss: 0.6365128755569458\n",
      "Patch: 62, Meta Train Loss: 0.6360860466957092\n",
      "Patch: 63, Meta Train Loss: 0.6341367363929749\n",
      "Patch: 64, Meta Train Loss: 0.6233140230178833\n",
      "Patch: 65, Meta Train Loss: 0.6700387001037598\n",
      "Patch: 66, Meta Train Loss: 0.6849339604377747\n",
      "Patch: 67, Meta Train Loss: 0.6919123530387878\n",
      "Patch: 68, Meta Train Loss: 0.6862974762916565\n",
      "Patch: 69, Meta Train Loss: 0.6413386464118958\n",
      "Patch: 70, Meta Train Loss: 0.6814091205596924\n",
      "Patch: 71, Meta Train Loss: 0.6328760981559753\n",
      "Patch: 72, Meta Train Loss: 0.6693633198738098\n",
      "Patch: 73, Meta Train Loss: 0.5433453321456909\n",
      "Patch: 74, Meta Train Loss: 0.682072103023529\n",
      "Patch: 75, Meta Train Loss: 0.5220010876655579\n",
      "Patch: 76, Meta Train Loss: 0.628770112991333\n",
      "Patch: 77, Meta Train Loss: 0.5837370753288269\n",
      "Patch: 78, Meta Train Loss: 0.6567279100418091\n",
      "Patch: 79, Meta Train Loss: 0.6289406418800354\n",
      "Patch: 80, Meta Train Loss: 0.6640912294387817\n",
      "Patch: 81, Meta Train Loss: 0.6303048729896545\n",
      "Patch: 82, Meta Train Loss: 0.6350054740905762\n",
      "Patch: 83, Meta Train Loss: 0.6828649640083313\n",
      "Patch: 84, Meta Train Loss: 0.6405724883079529\n",
      "Patch: 85, Meta Train Loss: 0.6853852868080139\n",
      "Patch: 86, Meta Train Loss: 0.6611684560775757\n",
      "Patch: 87, Meta Train Loss: 0.6422245502471924\n",
      "Patch: 88, Meta Train Loss: 0.6790593862533569\n",
      "Patch: 89, Meta Train Loss: 0.5985742807388306\n",
      "Patch: 90, Meta Train Loss: 0.6699745655059814\n",
      "Patch: 91, Meta Train Loss: 0.6229454278945923\n",
      "Patch: 92, Meta Train Loss: 0.6579335927963257\n",
      "Patch: 93, Meta Train Loss: 0.6429892778396606\n",
      "Patch: 94, Meta Train Loss: 0.6181597113609314\n",
      "Patch: 95, Meta Train Loss: 0.6296241879463196\n",
      "Patch: 96, Meta Train Loss: 0.6668180227279663\n",
      "Patch: 97, Meta Train Loss: 0.6882144808769226\n",
      "Patch: 98, Meta Train Loss: 0.6398774981498718\n",
      "Patch: 99, Meta Train Loss: 0.6823413372039795\n",
      "Patch: 100, Meta Train Loss: 0.6436620354652405\n",
      "Patch: 101, Meta Train Loss: 0.5828902721405029\n",
      "Patch: 102, Meta Train Loss: 0.6638770699501038\n",
      "Patch: 103, Meta Train Loss: 0.623178243637085\n",
      "Patch: 104, Meta Train Loss: 0.6367614269256592\n",
      "Patch: 105, Meta Train Loss: 0.5691516995429993\n",
      "Patch: 106, Meta Train Loss: 0.5873905420303345\n",
      "Patch: 107, Meta Train Loss: 0.6224474906921387\n",
      "Patch: 108, Meta Train Loss: 0.6362084746360779\n",
      "Patch: 109, Meta Train Loss: 0.6354401111602783\n",
      "Patch: 110, Meta Train Loss: 0.6323609948158264\n",
      "Patch: 111, Meta Train Loss: 0.6398702263832092\n",
      "Patch: 112, Meta Train Loss: 0.5955719351768494\n",
      "Patch: 113, Meta Train Loss: 0.6377971768379211\n",
      "Patch: 114, Meta Train Loss: 0.6532142758369446\n",
      "Patch: 115, Meta Train Loss: 0.5759032368659973\n",
      "Patch: 116, Meta Train Loss: 0.6659922003746033\n",
      "Patch: 117, Meta Train Loss: 0.5952321887016296\n",
      "Patch: 118, Meta Train Loss: 0.6131426095962524\n",
      "Patch: 119, Meta Train Loss: 0.6621308922767639\n",
      "Patch: 120, Meta Train Loss: 0.7306617498397827\n",
      "Patch: 121, Meta Train Loss: 0.620243489742279\n",
      "Patch: 122, Meta Train Loss: 0.6718719601631165\n",
      "Patch: 123, Meta Train Loss: 0.6148530840873718\n",
      "Patch: 124, Meta Train Loss: 0.6149449944496155\n",
      "Patch: 125, Meta Train Loss: 0.6107239723205566\n",
      "Patch: 126, Meta Train Loss: 0.6830633878707886\n",
      "Patch: 127, Meta Train Loss: 0.5566067099571228\n",
      "Patch: 128, Meta Train Loss: 0.6661219596862793\n",
      "Patch: 129, Meta Train Loss: 0.6251844763755798\n",
      "Patch: 130, Meta Train Loss: 0.590628445148468\n",
      "Patch: 131, Meta Train Loss: 0.5403779745101929\n",
      "Patch: 132, Meta Train Loss: 0.521171510219574\n",
      "Patch: 133, Meta Train Loss: 0.6067251563072205\n",
      "Patch: 134, Meta Train Loss: 0.6454558968544006\n",
      "Patch: 135, Meta Train Loss: 0.6785253882408142\n",
      "Patch: 136, Meta Train Loss: 0.6263836622238159\n",
      "Patch: 137, Meta Train Loss: 0.5676168203353882\n",
      "Patch: 138, Meta Train Loss: 0.6418762803077698\n",
      "Patch: 139, Meta Train Loss: 0.6849136352539062\n",
      "Patch: 140, Meta Train Loss: 0.6205328702926636\n",
      "Patch: 141, Meta Train Loss: 0.5217574238777161\n",
      "Patch: 142, Meta Train Loss: 0.6904442310333252\n",
      "Patch: 143, Meta Train Loss: 0.6512608528137207\n",
      "Patch: 144, Meta Train Loss: 0.5855526328086853\n",
      "Patch: 145, Meta Train Loss: 0.7123960256576538\n",
      "Patch: 146, Meta Train Loss: 0.7228310108184814\n",
      "Patch: 147, Meta Train Loss: 0.6525565385818481\n",
      "Patch: 148, Meta Train Loss: 0.5861919522285461\n",
      "Patch: 149, Meta Train Loss: 0.6179720759391785\n",
      "Patch: 150, Meta Train Loss: 0.5803589224815369\n",
      "Patch: 151, Meta Train Loss: 0.5176013708114624\n",
      "Patch: 152, Meta Train Loss: 0.5614751577377319\n",
      "Patch: 153, Meta Train Loss: 0.5769606232643127\n",
      "Patch: 154, Meta Train Loss: 0.578840434551239\n",
      "Patch: 155, Meta Train Loss: 0.5807077884674072\n",
      "Patch: 156, Meta Train Loss: 0.631791353225708\n",
      "Patch: 157, Meta Train Loss: 0.5808144211769104\n",
      "Patch: 158, Meta Train Loss: 0.6072603464126587\n",
      "Patch: 159, Meta Train Loss: 0.5773487687110901\n",
      "Patch: 160, Meta Train Loss: 0.549948513507843\n",
      "Patch: 161, Meta Train Loss: 0.5997885465621948\n",
      "Patch: 162, Meta Train Loss: 0.676718533039093\n",
      "Patch: 163, Meta Train Loss: 0.581678569316864\n",
      "Patch: 164, Meta Train Loss: 0.6396874785423279\n",
      "Patch: 165, Meta Train Loss: 0.5654352307319641\n",
      "Patch: 166, Meta Train Loss: 0.5305765867233276\n",
      "Patch: 167, Meta Train Loss: 0.5703442692756653\n",
      "Patch: 168, Meta Train Loss: 0.694610059261322\n",
      "Patch: 169, Meta Train Loss: 0.6542963981628418\n",
      "Patch: 170, Meta Train Loss: 0.6365126371383667\n",
      "Patch: 171, Meta Train Loss: 0.5553483366966248\n",
      "Patch: 172, Meta Train Loss: 0.6185516715049744\n",
      "Patch: 173, Meta Train Loss: 0.5348493456840515\n",
      "Patch: 174, Meta Train Loss: 0.6510120630264282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch: 175, Meta Train Loss: 0.5885035991668701\n",
      "Patch: 176, Meta Train Loss: 0.6503459811210632\n",
      "Patch: 177, Meta Train Loss: 0.6192144155502319\n",
      "Patch: 178, Meta Train Loss: 0.5800887942314148\n",
      "Patch: 179, Meta Train Loss: 0.7001382112503052\n",
      "Patch: 180, Meta Train Loss: 0.7011526823043823\n",
      "Patch: 181, Meta Train Loss: 0.6814320087432861\n",
      "Patch: 182, Meta Train Loss: 0.5755913257598877\n",
      "Patch: 183, Meta Train Loss: 0.5411161184310913\n",
      "Patch: 184, Meta Train Loss: 0.7367717623710632\n",
      "Patch: 185, Meta Train Loss: 0.6010003685951233\n",
      "Patch: 186, Meta Train Loss: 0.6404076218605042\n",
      "Patch: 187, Meta Train Loss: 0.6350972652435303\n",
      "Patch: 188, Meta Train Loss: 0.5611374378204346\n",
      "Patch: 189, Meta Train Loss: 0.5453050136566162\n",
      "Patch: 190, Meta Train Loss: 0.6219230890274048\n",
      "Patch: 191, Meta Train Loss: 0.7567082643508911\n",
      "Patch: 192, Meta Train Loss: 0.5734137892723083\n",
      "Patch: 193, Meta Train Loss: 0.4775654971599579\n",
      "Patch: 194, Meta Train Loss: 0.5325498580932617\n",
      "Patch: 195, Meta Train Loss: 0.5856764316558838\n",
      "Patch: 196, Meta Train Loss: 0.5990532040596008\n",
      "Patch: 197, Meta Train Loss: 0.7606188654899597\n",
      "Patch: 198, Meta Train Loss: 0.5147542953491211\n",
      "Patch: 199, Meta Train Loss: 0.621650755405426\n",
      "Patch: 200, Meta Train Loss: 0.5689409971237183\n",
      "Patch: 201, Meta Train Loss: 0.5803099274635315\n",
      "Patch: 202, Meta Train Loss: 0.6194442510604858\n",
      "Patch: 203, Meta Train Loss: 0.5495147705078125\n",
      "Patch: 204, Meta Train Loss: 0.7005435228347778\n",
      "Patch: 205, Meta Train Loss: 0.7952909469604492\n",
      "Patch: 206, Meta Train Loss: 0.6614975333213806\n",
      "Patch: 207, Meta Train Loss: 0.5954411625862122\n",
      "Patch: 208, Meta Train Loss: 0.5422371625900269\n",
      "Patch: 209, Meta Train Loss: 0.6048381924629211\n",
      "Patch: 210, Meta Train Loss: 0.6278520226478577\n",
      "Patch: 211, Meta Train Loss: 0.6295710206031799\n",
      "Patch: 212, Meta Train Loss: 0.6815003156661987\n",
      "Patch: 213, Meta Train Loss: 0.5801414847373962\n",
      "Patch: 214, Meta Train Loss: 0.6459447741508484\n",
      "Patch: 215, Meta Train Loss: 0.7031875252723694\n",
      "Patch: 216, Meta Train Loss: 0.6721370220184326\n",
      "Patch: 217, Meta Train Loss: 0.6137308478355408\n",
      "Patch: 218, Meta Train Loss: 0.6011154055595398\n",
      "Patch: 219, Meta Train Loss: 0.666363537311554\n",
      "Patch: 220, Meta Train Loss: 0.7873625159263611\n",
      "Patch: 221, Meta Train Loss: 0.4849584698677063\n",
      "Patch: 222, Meta Train Loss: 0.6871903538703918\n",
      "Patch: 223, Meta Train Loss: 0.6799514889717102\n",
      "Patch: 224, Meta Train Loss: 0.7077881097793579\n",
      "Patch: 225, Meta Train Loss: 0.6405344605445862\n",
      "Patch: 226, Meta Train Loss: 0.6110413074493408\n",
      "Patch: 227, Meta Train Loss: 0.6600905060768127\n",
      "Patch: 228, Meta Train Loss: 0.5451154708862305\n",
      "Patch: 229, Meta Train Loss: 0.5516027808189392\n",
      "Patch: 230, Meta Train Loss: 0.605786919593811\n",
      "Patch: 231, Meta Train Loss: 0.678978443145752\n",
      "Patch: 232, Meta Train Loss: 0.5056474208831787\n",
      "Patch: 233, Meta Train Loss: 0.5914064645767212\n",
      "Patch: 234, Meta Train Loss: 0.511731743812561\n",
      "Patch: 235, Meta Train Loss: 0.6832601428031921\n",
      "Patch: 236, Meta Train Loss: 0.5587541460990906\n",
      "Patch: 237, Meta Train Loss: 0.669717013835907\n",
      "Patch: 238, Meta Train Loss: 0.5859105587005615\n",
      "Patch: 239, Meta Train Loss: 0.6253167986869812\n",
      "Patch: 240, Meta Train Loss: 0.642029345035553\n",
      "Patch: 241, Meta Train Loss: 0.6201534867286682\n",
      "Patch: 242, Meta Train Loss: 0.5420911312103271\n",
      "Patch: 243, Meta Train Loss: 0.6270880699157715\n",
      "Patch: 244, Meta Train Loss: 0.5210872292518616\n",
      "Patch: 245, Meta Train Loss: 0.6448234915733337\n",
      "Patch: 246, Meta Train Loss: 0.4530333876609802\n",
      "Patch: 247, Meta Train Loss: 0.5613330602645874\n",
      "Patch: 248, Meta Train Loss: 0.6548455953598022\n",
      "Patch: 249, Meta Train Loss: 0.5382353663444519\n",
      "Patch: 250, Meta Train Loss: 0.6047684550285339\n",
      "Patch: 251, Meta Train Loss: 0.5970854759216309\n",
      "Patch: 252, Meta Train Loss: 0.7262928485870361\n",
      "Patch: 253, Meta Train Loss: 0.6114845871925354\n",
      "Patch: 254, Meta Train Loss: 0.5440314412117004\n",
      "Patch: 255, Meta Train Loss: 0.6672935485839844\n",
      "Patch: 256, Meta Train Loss: 0.604988157749176\n",
      "Patch: 257, Meta Train Loss: 0.6156224012374878\n",
      "Patch: 258, Meta Train Loss: 0.5447120070457458\n",
      "Patch: 259, Meta Train Loss: 0.5893234610557556\n",
      "Patch: 260, Meta Train Loss: 0.6653100848197937\n",
      "Patch: 261, Meta Train Loss: 0.7306991219520569\n",
      "Patch: 262, Meta Train Loss: 0.6856118440628052\n",
      "Patch: 263, Meta Train Loss: 0.6715166568756104\n",
      "Patch: 264, Meta Train Loss: 0.6303485035896301\n",
      "Patch: 265, Meta Train Loss: 0.5474629998207092\n",
      "Patch: 266, Meta Train Loss: 0.5815041065216064\n",
      "Patch: 267, Meta Train Loss: 0.684241533279419\n",
      "Patch: 268, Meta Train Loss: 0.7132432460784912\n",
      "Patch: 269, Meta Train Loss: 0.6002517342567444\n",
      "Patch: 270, Meta Train Loss: 0.5789834260940552\n",
      "Patch: 271, Meta Train Loss: 0.629753589630127\n",
      "Patch: 272, Meta Train Loss: 0.5789803266525269\n",
      "Patch: 273, Meta Train Loss: 0.6147927641868591\n",
      "Patch: 274, Meta Train Loss: 0.6217232346534729\n",
      "Patch: 275, Meta Train Loss: 0.7023392915725708\n",
      "Patch: 276, Meta Train Loss: 0.7192009091377258\n",
      "Patch: 277, Meta Train Loss: 0.6265013813972473\n",
      "Patch: 278, Meta Train Loss: 0.6935305595397949\n",
      "Patch: 279, Meta Train Loss: 0.6474129557609558\n",
      "Patch: 280, Meta Train Loss: 0.6827556490898132\n",
      "Patch: 281, Meta Train Loss: 0.5645721554756165\n",
      "Patch: 282, Meta Train Loss: 0.6094169020652771\n",
      "Patch: 283, Meta Train Loss: 0.6514851450920105\n",
      "Patch: 284, Meta Train Loss: 0.5678451061248779\n",
      "Patch: 285, Meta Train Loss: 0.6586132645606995\n",
      "Patch: 286, Meta Train Loss: 0.5961655974388123\n",
      "Patch: 287, Meta Train Loss: 0.6751149892807007\n",
      "Patch: 288, Meta Train Loss: 0.6572144031524658\n",
      "Patch: 289, Meta Train Loss: 0.7134800553321838\n",
      "Patch: 290, Meta Train Loss: 0.6237779259681702\n",
      "Patch: 291, Meta Train Loss: 0.6360181570053101\n",
      "Patch: 292, Meta Train Loss: 0.6370512247085571\n",
      "Patch: 293, Meta Train Loss: 0.7291110754013062\n",
      "Patch: 294, Meta Train Loss: 0.5488169193267822\n",
      "Patch: 295, Meta Train Loss: 0.5871478319168091\n",
      "Patch: 296, Meta Train Loss: 0.5952461361885071\n",
      "Patch: 297, Meta Train Loss: 0.7431118488311768\n",
      "Patch: 298, Meta Train Loss: 0.660749614238739\n",
      "Patch: 299, Meta Train Loss: 0.5622918605804443\n",
      "Patch: 300, Meta Train Loss: 0.5601264238357544\n"
     ]
    }
   ],
   "source": [
    "for counter in range(300):\n",
    "    t0 = time.time()\n",
    "    total_loss = 0.0\n",
    "    all_train_task = []\n",
    "    for i in range(0,len(train_tasks)):\n",
    "        cell_task = train_tasks[i]\n",
    "        cell_task  =label_sampling(cell_task,25)\n",
    "        pos = cell_task[cell_task['label'] == 1]\n",
    "        neg = cell_task[cell_task['label'] == 0]\n",
    "        pos_train, pos_test = train_test_split(pos, test_size=0.5)\n",
    "        neg_test, neg_train = train_test_split(neg, test_size=0.5) ##Balanced positive and negative sample numbers\n",
    "        train = pd.concat([pos_train,neg_train],axis = 0)\n",
    "        test = pd.concat([pos_test,neg_test],axis = 0)\n",
    "        temp_dict={'train':train ,\n",
    "               'test':test,\n",
    "              }\n",
    "        all_train_task.append(temp_dict)\n",
    "        \n",
    "    for j,d in enumerate(all_train_task):\n",
    "        net_copy = maml.clone()\n",
    "        support_set = d['train']\n",
    "        querry_set = d['test']\n",
    "        supportdata = DrugCombDataset(support_set,drug_features,cell_features)\n",
    "        querrydata = DrugCombDataset(querry_set,drug_features,cell_features)\n",
    "        support_loader = DataLoader(supportdata, batch_size=25, shuffle=True)\n",
    "        querry_loader = DataLoader(querrydata, batch_size=25, shuffle=True)\n",
    "        \n",
    "        ##Gradient descent of support sets\n",
    "        for _ in range(1):\n",
    "            for iteration, sample in enumerate(support_loader):\n",
    "                support_d1=Variable(sample['d1'])\n",
    "                support_d1_sm = Variable(sample['d1_sm'])\n",
    "\n",
    "        \n",
    "                support_d2=Variable(sample['d2'])\n",
    "                support_d2_sm = Variable(sample['d2_sm'])\n",
    "\n",
    "\n",
    "        \n",
    "                support_cell = Variable(sample['cell'])\n",
    "                support_c_ge = Variable(sample['c_ge'].float())\n",
    "        \n",
    "                support_label = Variable(sample['label'].float())\n",
    "        \n",
    "        \n",
    "                support_d1=support_d1.to(device)\n",
    "                support_d1_sm = support_d1_sm.to(device)\n",
    "\n",
    "         \n",
    "                support_d2=support_d2.to(device)\n",
    "                support_d2_sm = support_d2_sm.to(device)\n",
    "\n",
    "                support_cell = support_cell.to(device)\n",
    "                support_c_ge = support_c_ge.to(device)\n",
    "        \n",
    "                support_label = support_label.to(device)\n",
    "                support_pred = net_meta((support_d1,support_d1_sm), (support_d2, support_d2_sm), (support_cell,support_c_ge))\n",
    "                support_loss = criterion(support_pred, support_label.view(-1,1))\n",
    "                support_pred = support_pred.cpu().detach().numpy()\n",
    "                support_label = support_label.view(-1,1).cpu().detach().numpy()\n",
    "    \n",
    "                net_copy.adapt(support_loss)\n",
    "                \n",
    "        ##Accumulating loss of query sets    \n",
    "        for iteration, sample in enumerate(querry_loader):\n",
    "            querry_d1=Variable(sample['d1'])\n",
    "            querry_d1_sm = Variable(sample['d1_sm'])\n",
    "\n",
    "        \n",
    "            querry_d2=Variable(sample['d2'])\n",
    "            querry_d2_sm = Variable(sample['d2_sm'])\n",
    "\n",
    "\n",
    "        \n",
    "            querry_cell = Variable(sample['cell'])\n",
    "            querry_c_ge = Variable(sample['c_ge'].float())\n",
    "        \n",
    "            querry_label = Variable(sample['label'].float())\n",
    "        \n",
    "        \n",
    "            querry_d1=querry_d1.to(device)\n",
    "            querry_d1_sm = querry_d1_sm.to(device)\n",
    "\n",
    "         \n",
    "            querry_d2=querry_d2.to(device)\n",
    "            querry_d2_sm = querry_d2_sm.to(device)\n",
    "\n",
    "\n",
    "        \n",
    "            querry_cell = querry_cell.to(device)\n",
    "            querry_c_ge = querry_c_ge.to(device)\n",
    "        \n",
    "            querry_label = querry_label.to(device)\n",
    "            querry_pred = net_meta((querry_d1,querry_d1_sm), (querry_d2, querry_d2_sm), (querry_cell,querry_c_ge))\n",
    "            querry_loss = criterion(querry_pred, querry_label.view(-1,1))\n",
    "            querry_pred = querry_pred.cpu().detach().numpy()\n",
    "            querry_label = querry_label.view(-1,1).cpu().detach().numpy()\n",
    "            total_loss += querry_loss\n",
    "                \n",
    "    meta_loss = querry_loss\n",
    "    \n",
    "    print(f\"Patch: {counter+1}, Meta Train Loss: {meta_loss.item()}\")\n",
    "    meta_optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    meta_optimizer.step()\n",
    "\n",
    "    \n",
    "torch.save(net_meta.state_dict(), \"E:/result/meta_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb4eda",
   "metadata": {},
   "source": [
    "## Meta-testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85d9a7",
   "metadata": {},
   "source": [
    "### challeng 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28caa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide tasks according to cell line  types\n",
    "test_tasks = []\n",
    "cell_lines = target_data['cell_line_name'].unique()\n",
    "for cell_line in cell_lines:\n",
    "    task = target_data[target_data['cell_line_name'] == cell_line]\n",
    "    test_tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "abcfba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN auroc of the model on all the task data: 0.750177\n",
      "MEAN auor of the model on all the task data: 0.723584\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "all_roc = []\n",
    "all_pr = []\n",
    "for i in range (0,100):\n",
    "    all_test_task = []\n",
    "    for t in range(0,len(test_tasks)):\n",
    "        cell_task = test_tasks[t]\n",
    "        train, test = train_test_split(cell_task, test_size=0.5)\n",
    "        temp_dict={'train':train,\n",
    "               'test':test,\n",
    "              }\n",
    "        all_test_task.append(temp_dict)\n",
    "    for j,d in enumerate(all_test_task):\n",
    "        net_meta = Comb().to(device)\n",
    "        net_meta.load_state_dict(torch.load(\"E:/result/meta_model.pth\"))\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(net_meta.parameters(), lr=0.001)\n",
    "        support_set = d['train']\n",
    "        querry_set = d['test']\n",
    "        supportdata = DrugCombDataset(support_set,drug_features,cell_features)\n",
    "        querrydata = DrugCombDataset(querry_set,drug_features,cell_features)\n",
    "        support_loader = DataLoader(supportdata, batch_size=len(support_set), shuffle=True)\n",
    "        querry_loader = DataLoader(querrydata, batch_size=len(querry_set), shuffle=True)\n",
    "        \n",
    "    #fine-tuning meta model\n",
    "        net_meta.train()\n",
    "        for epoch in range(3):  # 3 gradient descent of support sets\n",
    "            for iteration, sample in enumerate(support_loader):\n",
    "                support_d1=Variable(sample['d1'])\n",
    "                support_d1_sm = Variable(sample['d1_sm'])\n",
    "\n",
    "\n",
    "        \n",
    "                support_d2=Variable(sample['d2'])\n",
    "                support_d2_sm = Variable(sample['d2_sm'])\n",
    "     \n",
    "\n",
    "        \n",
    "                support_cell = Variable(sample['cell'])\n",
    "                support_c_ge = Variable(sample['c_ge'].float())\n",
    "        \n",
    "                support_label = Variable(sample['label'].float())\n",
    "        \n",
    "        \n",
    "                support_d1=support_d1.to(device)\n",
    "                support_d1_sm = support_d1_sm.to(device)\n",
    "\n",
    "         \n",
    "                support_d2=support_d2.to(device)\n",
    "                support_d2_sm = support_d2_sm.to(device)\n",
    "\n",
    "        \n",
    "                support_cell = support_cell.to(device)\n",
    "                support_c_ge = support_c_ge.to(device)\n",
    "        \n",
    "                support_label = support_label.to(device)\n",
    "                support_pred = net_meta((support_d1,support_d1_sm), (support_d2, support_d2_sm), (support_cell,support_c_ge))\n",
    "                support_loss = criterion(support_pred, support_label.view(-1,1))\n",
    "                support_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "     # model-predicting\n",
    "        net_meta.eval()\n",
    "        with torch.no_grad():\n",
    "            for iteration, sample in enumerate(querry_loader):\n",
    "                querry_d1=Variable(sample['d1'])\n",
    "                querry_d1_sm = Variable(sample['d1_sm'])\n",
    "\n",
    "                querry_d2=Variable(sample['d2'])\n",
    "                querry_d2_sm = Variable(sample['d2_sm'])\n",
    "\n",
    "        \n",
    "                querry_cell = Variable(sample['cell'])\n",
    "                querry_c_ge = Variable(sample['c_ge'].float())\n",
    "        \n",
    "                querry_label = Variable(sample['label'].float())\n",
    "        \n",
    "        \n",
    "                querry_d1=querry_d1.to(device)\n",
    "                querry_d1_sm = querry_d1_sm.to(device)\n",
    "\n",
    "                querry_d2=querry_d2.to(device)\n",
    "                querry_d2_sm = querry_d2_sm.to(device)\n",
    "\n",
    "                querry_cell = querry_cell.to(device)\n",
    "                querry_c_ge = querry_c_ge.to(device)\n",
    "        \n",
    "                querry_label = querry_label.to(device)\n",
    "                querry_pred = net_meta((querry_d1,querry_d1_sm), (querry_d2, querry_d2_sm), (querry_cell,querry_c_ge))\n",
    "                preds.extend(querry_pred.cpu().detach().numpy())\n",
    "                labels.extend(querry_label.cpu().detach().numpy())\n",
    "                \n",
    "                \n",
    "                \n",
    "    fpr, tpr, thersholds = roc_curve(labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "    aupr = auc(recall, precision)\n",
    "\n",
    "\n",
    "\n",
    "    all_roc.append(roc_auc)\n",
    "    all_pr.append(aupr)\n",
    "    \n",
    "mean_roc = np.mean(all_roc)\n",
    "mean_pr = np.mean(all_pr)\n",
    "\n",
    "\n",
    "print('MEAN auroc of the model on all the task data: %f' % mean_roc)   \n",
    "print('MEAN auor of the model on all the task data: %f' % mean_pr)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "L2LMAML",
   "language": "python",
   "name": "l2lmaml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
